{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6488923,"sourceType":"datasetVersion","datasetId":3749709}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ðŸ“Œ Install necessary packages\n!pip install transformers datasets sentencepiece\n\n# âœ… Imports\nfrom transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\nfrom datasets import load_dataset, Dataset\nimport pandas as pd\n\n# âœ… Load and prepare dataset\ndf = pd.read_csv(\"/kaggle/input/fake-reviews-dataset/fake reviews dataset.csv\")\ndf = df.dropna(subset=['text_', 'label'])\n\n# âœ… Generate explanation-based summary\ndef generate_target(row):\n    if row['label'] == 1:\n        return \"Summary: \" + row['text_'][:100] + \" ... This review might be fake due to vague wording or repetition.\"\n    else:\n        return \"Summary: \" + row['text_'][:100] + \" ... This review appears genuine and specific.\"\n\ndf['target'] = df.apply(generate_target, axis=1)\n\n# âœ… Convert to Hugging Face dataset\ndataset = Dataset.from_pandas(df[['text_', 'target']].rename(columns={\"text_\": \"text\", \"target\": \"summary\"}))\ndataset = dataset.train_test_split(test_size=0.1)\n\n# âœ… Tokenizer and model\nmodel_name = \"t5-small\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# âœ… Preprocessing function\ndef preprocess_function(examples):\n    inputs = [\"summarize: \" + text for text in examples[\"text\"]]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n\n    labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized = dataset.map(preprocess_function, batched=True)\n\n# âœ… Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    save_total_limit=2,\n)\n\n# âœ… Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"test\"],\n    tokenizer=tokenizer,\n)\n\n# âœ… Train the model\ntrainer.train()\n\n# âœ… Save the model\nmodel.save_pretrained(\"t5-fake-review-summarizer\")\ntokenizer.save_pretrained(\"t5-fake-review-summarizer\")\n\n# âœ… Load and summarize new review using fine-tuned model\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-fake-review-summarizer\")\ntokenizer = T5Tokenizer.from_pretrained(\"t5-fake-review-summarizer\")\n\ntext = \"This product changed my life. It is the best ever. Highly recommended!!!\"\ninput_text = \"summarize: \" + text\ninput_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n\noutput = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T22:51:24.102422Z","iopub.execute_input":"2025-04-17T22:51:24.102935Z","iopub.status.idle":"2025-04-17T23:43:13.985973Z","shell.execute_reply.started":"2025-04-17T22:51:24.102896Z","shell.execute_reply":"2025-04-17T23:43:13.985234Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"2025-04-17 22:51:30.802425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744930290.824752      99 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744930290.831698      99 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36388 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6230d0aa7f2f4280ad50af56a0ed3769"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4044 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69a714c3724340d8a93da2cc227efd9b"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_99/4217245742.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6825' max='6825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6825/6825 51:04, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.044700</td>\n      <td>0.048714</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.036200</td>\n      <td>0.040350</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.034300</td>\n      <td>0.040062</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Summary: This product changed my life. It is the best ever. Highly recommended!!!... This review appears genuine and specific.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T22:51:15.047953Z","iopub.execute_input":"2025-04-17T22:51:15.048186Z","iopub.status.idle":"2025-04-17T22:51:15.051583Z","shell.execute_reply.started":"2025-04-17T22:51:15.048168Z","shell.execute_reply":"2025-04-17T22:51:15.051036Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!zip -r t5-fake-review-summarizer.zip t5-fake-review-summarizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T23:47:00.628265Z","iopub.execute_input":"2025-04-17T23:47:00.628840Z","iopub.status.idle":"2025-04-17T23:47:13.847103Z","shell.execute_reply.started":"2025-04-17T23:47:00.628814Z","shell.execute_reply":"2025-04-17T23:47:13.846362Z"}},"outputs":[{"name":"stdout","text":"  adding: t5-fake-review-summarizer/ (stored 0%)\n  adding: t5-fake-review-summarizer/added_tokens.json (deflated 83%)\n  adding: t5-fake-review-summarizer/generation_config.json (deflated 29%)\n  adding: t5-fake-review-summarizer/special_tokens_map.json (deflated 85%)\n  adding: t5-fake-review-summarizer/config.json (deflated 62%)\n  adding: t5-fake-review-summarizer/model.safetensors (deflated 9%)\n  adding: t5-fake-review-summarizer/spiece.model (deflated 48%)\n  adding: t5-fake-review-summarizer/tokenizer_config.json (deflated 94%)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!zip -r results.zip results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T23:47:51.546931Z","iopub.execute_input":"2025-04-17T23:47:51.547299Z","iopub.status.idle":"2025-04-17T23:49:03.433618Z","shell.execute_reply.started":"2025-04-17T23:47:51.547273Z","shell.execute_reply":"2025-04-17T23:49:03.432641Z"}},"outputs":[{"name":"stdout","text":"  adding: results/ (stored 0%)\n  adding: results/checkpoint-6500/ (stored 0%)\n  adding: results/checkpoint-6500/added_tokens.json (deflated 83%)\n  adding: results/checkpoint-6500/rng_state.pth (deflated 25%)\n  adding: results/checkpoint-6500/trainer_state.json (deflated 73%)\n  adding: results/checkpoint-6500/generation_config.json (deflated 29%)\n  adding: results/checkpoint-6500/optimizer.pt (deflated 7%)\n  adding: results/checkpoint-6500/scheduler.pt (deflated 55%)\n  adding: results/checkpoint-6500/special_tokens_map.json (deflated 85%)\n  adding: results/checkpoint-6500/config.json (deflated 62%)\n  adding: results/checkpoint-6500/training_args.bin (deflated 51%)\n  adding: results/checkpoint-6500/model.safetensors (deflated 9%)\n  adding: results/checkpoint-6500/spiece.model (deflated 48%)\n  adding: results/checkpoint-6500/tokenizer_config.json (deflated 94%)\n  adding: results/checkpoint-6825/ (stored 0%)\n  adding: results/checkpoint-6825/added_tokens.json (deflated 83%)\n  adding: results/checkpoint-6825/rng_state.pth (deflated 25%)\n  adding: results/checkpoint-6825/trainer_state.json (deflated 72%)\n  adding: results/checkpoint-6825/generation_config.json (deflated 29%)\n  adding: results/checkpoint-6825/optimizer.pt (deflated 7%)\n  adding: results/checkpoint-6825/scheduler.pt (deflated 56%)\n  adding: results/checkpoint-6825/special_tokens_map.json (deflated 85%)\n  adding: results/checkpoint-6825/config.json (deflated 62%)\n  adding: results/checkpoint-6825/training_args.bin (deflated 51%)\n  adding: results/checkpoint-6825/model.safetensors (deflated 9%)\n  adding: results/checkpoint-6825/spiece.model (deflated 48%)\n  adding: results/checkpoint-6825/tokenizer_config.json (deflated 94%)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}