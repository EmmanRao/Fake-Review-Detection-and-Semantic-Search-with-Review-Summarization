{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6488923,"sourceType":"datasetVersion","datasetId":3749709}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade transformers datasets;","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:22:09.361570Z","iopub.execute_input":"2025-04-17T19:22:09.361920Z","iopub.status.idle":"2025-04-17T19:22:22.105641Z","shell.execute_reply.started":"2025-04-17T19:22:09.361870Z","shell.execute_reply":"2025-04-17T19:22:22.104938Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nCollecting transformers\n  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, transformers\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.1\n    Uninstalling transformers-4.51.1:\n      Successfully uninstalled transformers-4.51.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0 transformers-4.51.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:22:22.107409Z","iopub.execute_input":"2025-04-17T19:22:22.107639Z","iopub.status.idle":"2025-04-17T19:22:22.111599Z","shell.execute_reply.started":"2025-04-17T19:22:22.107617Z","shell.execute_reply":"2025-04-17T19:22:22.110929Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport torch\nimport shutil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:32:38.731858Z","iopub.execute_input":"2025-04-17T19:32:38.732617Z","iopub.status.idle":"2025-04-17T19:32:38.736383Z","shell.execute_reply.started":"2025-04-17T19:32:38.732592Z","shell.execute_reply":"2025-04-17T19:32:38.735670Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"#uploaded = files.upload()\nfile_path =  \"/kaggle/input/fake-reviews-dataset/fake reviews dataset.csv\"\n\ndef load_kaggle_dataset(file_path):\n    df = pd.read_csv(file_path)\n    df.columns = [col.strip().lower() for col in df.columns]\n    print(\"Columns found:\", df.columns.tolist())\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:32:38.909656Z","iopub.execute_input":"2025-04-17T19:32:38.909919Z","iopub.status.idle":"2025-04-17T19:32:38.914075Z","shell.execute_reply.started":"2025-04-17T19:32:38.909870Z","shell.execute_reply":"2025-04-17T19:32:38.913312Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Text cleaning\ndef preprocess_text(text):\n    if isinstance(text, str):\n        text = text.lower()\n        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n    return \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:32:39.036657Z","iopub.execute_input":"2025-04-17T19:32:39.036858Z","iopub.status.idle":"2025-04-17T19:32:39.040766Z","shell.execute_reply.started":"2025-04-17T19:32:39.036842Z","shell.execute_reply":"2025-04-17T19:32:39.040053Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Label conversion\ndef assign_labels(df):\n    if 'label' not in df.columns or 'text_' not in df.columns:\n        raise ValueError(\"Dataset must contain 'text_' and 'label' columns. Found: {}\".format(df.columns.tolist()))\n\n    label_mapping = {'OR': 0, 'CG': 1}\n    df = df[df['text_'].notnull()]\n    df['label'] = df['label'].map(label_mapping)\n\n    if df['label'].isnull().any():\n        raise ValueError(\"Label conversion failed — check for invalid labels in your data.\")\n\n    df['text'] = df['text_'].apply(preprocess_text)\n    return df[['label', 'text']]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:32:39.152849Z","iopub.execute_input":"2025-04-17T19:32:39.153088Z","iopub.status.idle":"2025-04-17T19:32:39.157788Z","shell.execute_reply.started":"2025-04-17T19:32:39.153072Z","shell.execute_reply":"2025-04-17T19:32:39.157114Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Train-validation-test split\ndef split_dataset(df):\n    train, temp = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n    val, test = train_test_split(temp, test_size=0.5, stratify=temp['label'], random_state=42)\n    return train, val, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:32:39.348470Z","iopub.execute_input":"2025-04-17T19:32:39.348975Z","iopub.status.idle":"2025-04-17T19:32:39.352542Z","shell.execute_reply.started":"2025-04-17T19:32:39.348950Z","shell.execute_reply":"2025-04-17T19:32:39.351925Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Tokenization and formatting for Hugging Face Datasets\ndef prepare_hf_dataset(train, val, test):\n    tokenizer = BertTokenizer.from_pretrained('SravaniNirati/bert_fake_review_detection')\n\n    def tokenize(batch):\n        return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=512)\n\n    train = train.sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    train_dataset = Dataset.from_pandas(train)\n    val_dataset = Dataset.from_pandas(val)\n    test_dataset = Dataset.from_pandas(test)\n\n    train_dataset = train_dataset.map(tokenize, batched=True)\n    val_dataset = val_dataset.map(tokenize, batched=True)\n    test_dataset = test_dataset.map(tokenize, batched=True)\n\n    columns_to_return = ['input_ids', 'attention_mask', 'label']\n    train_dataset.set_format(type='torch', columns=columns_to_return)\n    val_dataset.set_format(type='torch', columns=columns_to_return)\n    test_dataset.set_format(type='torch', columns=columns_to_return)\n\n    return train_dataset, val_dataset, test_dataset, tokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:32:39.471568Z","iopub.execute_input":"2025-04-17T19:32:39.471744Z","iopub.status.idle":"2025-04-17T19:32:39.477115Z","shell.execute_reply.started":"2025-04-17T19:32:39.471730Z","shell.execute_reply":"2025-04-17T19:32:39.476433Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {'accuracy': accuracy_score(labels, predictions)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:32:39.665415Z","iopub.execute_input":"2025-04-17T19:32:39.665958Z","iopub.status.idle":"2025-04-17T19:32:39.669604Z","shell.execute_reply.started":"2025-04-17T19:32:39.665932Z","shell.execute_reply":"2025-04-17T19:32:39.668945Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Evaluate pretrained model and return instance for saving\ndef evaluate_pretrained_model(test_dataset):\n    model = BertForSequenceClassification.from_pretrained('SravaniNirati/bert_fake_review_detection')\n    trainer = Trainer(model=model, eval_dataset=test_dataset, compute_metrics=compute_metrics)\n    results = trainer.evaluate()\n    print(\"Evaluation Results:\", results)\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:32:39.827841Z","iopub.execute_input":"2025-04-17T19:32:39.828089Z","iopub.status.idle":"2025-04-17T19:32:39.832131Z","shell.execute_reply.started":"2025-04-17T19:32:39.828072Z","shell.execute_reply":"2025-04-17T19:32:39.831459Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def fine_tune_model(train_dataset, val_dataset, tokenizer):\n    model = BertForSequenceClassification.from_pretrained('SravaniNirati/bert_fake_review_detection')\n\n    training_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=4,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    save_steps=200,\n    eval_steps=200,\n    save_total_limit=1\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:32:40.021816Z","iopub.execute_input":"2025-04-17T19:32:40.022717Z","iopub.status.idle":"2025-04-17T19:32:40.028836Z","shell.execute_reply.started":"2025-04-17T19:32:40.022689Z","shell.execute_reply":"2025-04-17T19:32:40.028055Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Save model locally (Kaggle version)\ndef save_and_download_model(model, tokenizer, path='bert_fake_review_model'):\n    model.save_pretrained(path)\n    tokenizer.save_pretrained(path)\n    shutil.make_archive(path, 'zip', path)\n    print(f\"✅ Model saved and zipped at: {path}.zip — You can download it from the right-side file panel.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:32:40.193038Z","iopub.execute_input":"2025-04-17T19:32:40.193609Z","iopub.status.idle":"2025-04-17T19:32:40.199498Z","shell.execute_reply.started":"2025-04-17T19:32:40.193581Z","shell.execute_reply":"2025-04-17T19:32:40.198587Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Main execution flow\nif __name__ == \"__main__\":\n    file_path = \"/kaggle/input/fake-reviews-dataset/fake reviews dataset.csv\"  \n    df = load_kaggle_dataset(file_path)\n    df_clean = assign_labels(df)\n    train, val, test = split_dataset(df_clean)\n    train_dataset, val_dataset, test_dataset, tokenizer = prepare_hf_dataset(train, val, test)\n    model = evaluate_pretrained_model(test_dataset)\n    save_and_download_model(model, tokenizer)\n    print(\"✅ Evaluation complete and model zipped for download!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:33:07.653369Z","iopub.execute_input":"2025-04-17T19:33:07.653643Z","iopub.status.idle":"2025-04-17T19:35:51.734868Z","shell.execute_reply.started":"2025-04-17T19:33:07.653621Z","shell.execute_reply":"2025-04-17T19:35:51.734061Z"}},"outputs":[{"name":"stdout","text":"Columns found: ['category', 'rating', 'label', 'text_']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/28302 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32c8e948c0c54187b1cf129ff6f496ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6065 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21c597a903ea40548370f65dd93ebaa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6065 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c56384bfe4b4b6099ebee0efe4ff264"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='759' max='759' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [759/759 01:28]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 5.204216003417969, 'eval_model_preparation_time': 0.0026, 'eval_accuracy': 0.20131904369332235, 'eval_runtime': 89.1135, 'eval_samples_per_second': 68.059, 'eval_steps_per_second': 8.517}\n✅ Model saved and zipped at: bert_fake_review_model.zip — You can download it from the right-side file panel.\n✅ Evaluation complete and model zipped for download!\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# !pip install -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:32:48.633868Z","iopub.status.idle":"2025-04-17T19:32:48.634182Z","shell.execute_reply.started":"2025-04-17T19:32:48.634043Z","shell.execute_reply":"2025-04-17T19:32:48.634057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main Execution\nif __name__ == \"__main__\":\n    file_path = \"/kaggle/input/fake-reviews-dataset/fake reviews dataset.csv\"\n    df = load_kaggle_dataset(file_path)\n\n    # Inspect label distribution (optional)\n    print(\"\\nLabel distribution:\\n\", df['label'].value_counts())\n\n    df_clean = assign_labels(df)\n    train, val, test = split_dataset(df_clean)\n    train_dataset, val_dataset, test_dataset, tokenizer = prepare_hf_dataset(train, val, test)\n\n    model = fine_tune_model(train_dataset, val_dataset, tokenizer)\n\n    # Evaluate on test set\n    trainer = Trainer(model=model, eval_dataset=test_dataset, compute_metrics=compute_metrics)\n    test_results = trainer.evaluate()\n    print(\"📊 Test Accuracy after Fine-tuning:\", test_results)\n\n    save_and_download_model(model, tokenizer)\n    print(\"✅ All done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T19:35:51.736148Z","iopub.execute_input":"2025-04-17T19:35:51.736460Z","iopub.status.idle":"2025-04-17T21:39:15.850748Z","shell.execute_reply.started":"2025-04-17T19:35:51.736408Z","shell.execute_reply":"2025-04-17T21:39:15.849837Z"}},"outputs":[{"name":"stdout","text":"Columns found: ['category', 'rating', 'label', 'text_']\n\nLabel distribution:\n label\nCG    20216\nOR    20216\nName: count, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/28302 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ef780e5f414cd68a5cad05ac19eae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6065 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddb8184a93f146358f64006812de2b3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6065 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1899a6da92040da88ac89f937e9364b"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_31/1765776682.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7076' max='7076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7076/7076 2:00:37, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.519000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.212200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.225600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.157400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.126400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.125400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.134300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.140200</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.131100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.116100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.114900</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.111300</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.105400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.071100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.114500</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.092100</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.095900</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.072900</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.070100</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.051200</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.044700</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.042300</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.048100</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.047300</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.052300</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.041500</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.057500</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.052500</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.035800</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.041500</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.056600</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.032300</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.036800</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.036200</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.044600</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.037600</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.023900</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.020700</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.016000</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.017500</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.013600</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.020600</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.008000</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.009200</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.035100</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.027000</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.024600</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.005200</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.008800</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.004900</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.010300</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.006000</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.008200</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.006400</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.008500</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>0.004400</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.007300</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>0.005000</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.002200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='759' max='759' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [759/759 01:30]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"📊 Test Accuracy after Fine-tuning: {'eval_loss': 0.2128247320652008, 'eval_model_preparation_time': 0.0027, 'eval_accuracy': 0.9699917559769168, 'eval_runtime': 90.5011, 'eval_samples_per_second': 67.016, 'eval_steps_per_second': 8.387}\n✅ Model saved and zipped at: bert_fake_review_model.zip — You can download it from the right-side file panel.\n✅ All done!\n","output_type":"stream"}],"execution_count":34}]}